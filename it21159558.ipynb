{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "884fd282",
   "metadata": {},
   "source": [
    "## Data Preparation, Model Design, Training, and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e801fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Load the dataset\n",
    "with open(\"Sequential_Dataset.txt\", \"r\") as file:\n",
    "    text_data = file.read()\n",
    "\n",
    "# Create character-to-index and index-to-character mappings\n",
    "chars = sorted(list(set(text_data)))\n",
    "char_to_index = dict((c, i) for i, c in enumerate(chars))\n",
    "index_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "SEQUENCE_LENGTH = 20\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "for i in range(0, len(text_data) - SEQUENCE_LENGTH):\n",
    "    sequence_in = text_data[i:i + SEQUENCE_LENGTH]\n",
    "    sequence_out = text_data[i + SEQUENCE_LENGTH]\n",
    "    inputs.append([char_to_index[char] for char in sequence_in])\n",
    "    outputs.append(char_to_index[sequence_out])\n",
    "\n",
    "num_chars = len(chars)\n",
    "\n",
    "# Convert inputs and outputs for model\n",
    "X = np.reshape(inputs, (len(inputs), SEQUENCE_LENGTH, 1))\n",
    "X = X / float(num_chars)  # Normalize\n",
    "y = to_categorical(outputs)\n",
    "\n",
    "# LSTM Model Design\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_chars, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=15, batch_size=150)\n",
    "\n",
    "# Predict the next 5 characters for the 5th training record\n",
    "initial_sequence = text_data[4:24]\n",
    "print(\"Initial Sequence: \", initial_sequence)\n",
    "for i in range(5):\n",
    "    sequence = [char_to_index[char] for char in initial_sequence]\n",
    "    sequence = np.reshape(sequence, (1, len(sequence), 1))\n",
    "    sequence = sequence / float(num_chars)\n",
    "    prediction = model.predict(sequence, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    initial_sequence += index_to_char[index]\n",
    "    initial_sequence = initial_sequence[1:]\n",
    "print(\"Predicted Sequence: \", initial_sequence)\n",
    "\n",
    "# Hyperparameter tuning using grid search\n",
    "def create_model(dropout_rate=0.2, lstm_units=256, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(lstm_units, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(lstm_units))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_chars, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model)\n",
    "\n",
    "# Define the grid search parameters\n",
    "batch_size = [150, 200]\n",
    "epochs = [15, 20]\n",
    "dropout_rate = [0.1, 0.2]\n",
    "lstm_units = [256, 512]\n",
    "optimizer = ['SGD', 'Adam']\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs, dropout_rate=dropout_rate, lstm_units=lstm_units, optimizer=optimizer)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eb2701",
   "metadata": {},
   "source": [
    "\n",
    "## Hyperparameter Tuning:\n",
    "\n",
    "In the above code, we used grid search to find the optimal hyperparameters for our LSTM model. The parameters we tuned include:\n",
    "- Batch size\n",
    "- Epochs\n",
    "- Dropout rate\n",
    "- LSTM units\n",
    "- Optimizer\n",
    "\n",
    "The results from the grid search provide the best combination of these hyperparameters for the highest accuracy on the validation set.\n",
    "\n",
    "## Presentation and Documentation:\n",
    "\n",
    "The notebook provides a structured approach to the task of character generation using LSTM. The code is commented for clarity, and each section is demarcated for better understanding.\n",
    "\n",
    "For a detailed report:\n",
    "1. **Introduction**: Brief overview of the project and objectives.\n",
    "2. **Data Exploration**: Characteristics of the dataset.\n",
    "3. **Model Design and Training**: About the chosen model architecture and training process.\n",
    "4. **Hyperparameter Tuning**: Method used for tuning and best parameters obtained.\n",
    "5. **Results**: Display the initial 10 training sequences and their respective outputs. Show the 5-character prediction for the 5th training record.\n",
    "6. **Conclusion**: Summary of findings, challenges faced during the project, lessons learned, and future recommendations.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
